{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff4b253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d82a11c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08177661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'wiki_intro', 'generated_intro', 'title_len', 'wiki_intro_len', 'generated_intro_len', 'prompt', 'generated_text', 'prompt_tokens', 'generated_text_tokens'],\n",
       "    num_rows: 150000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b30b480",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa6f0b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>wiki_intro</th>\n",
       "      <th>generated_intro</th>\n",
       "      <th>title_len</th>\n",
       "      <th>wiki_intro_len</th>\n",
       "      <th>generated_intro_len</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generated_text</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>generated_text_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63064638</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Sexhow%20railway...</td>\n",
       "      <td>Sexhow railway station</td>\n",
       "      <td>Sexhow railway station was a railway station b...</td>\n",
       "      <td>Sexhow railway station was a railway station l...</td>\n",
       "      <td>3</td>\n",
       "      <td>174</td>\n",
       "      <td>78</td>\n",
       "      <td>200 word wikipedia style introduction on 'Sexh...</td>\n",
       "      <td>located in the town of Sexhow, on the Cumbria...</td>\n",
       "      <td>25</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>279621</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Eti%C3%A4inen</td>\n",
       "      <td>Etiäinen</td>\n",
       "      <td>In Finnish folklore, all places and things, an...</td>\n",
       "      <td>In Finnish folklore, all places and things, an...</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>80</td>\n",
       "      <td>200 word wikipedia style introduction on 'Etiä...</td>\n",
       "      <td>animate or inanimate, have a spirit or \"etiäi...</td>\n",
       "      <td>26</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>287229</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Inverse%20functi...</td>\n",
       "      <td>Inverse function theorem</td>\n",
       "      <td>In mathematics, specifically differential calc...</td>\n",
       "      <td>In mathematics, specifically differential calc...</td>\n",
       "      <td>3</td>\n",
       "      <td>170</td>\n",
       "      <td>59</td>\n",
       "      <td>200 word wikipedia style introduction on 'Inve...</td>\n",
       "      <td>function theorem states that for every real-v...</td>\n",
       "      <td>26</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26712375</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Stepping%20on%20...</td>\n",
       "      <td>Stepping on Roses</td>\n",
       "      <td>is a Japanese shōjo manga series written and i...</td>\n",
       "      <td>is a Japanese shōjo manga series written and i...</td>\n",
       "      <td>3</td>\n",
       "      <td>335</td>\n",
       "      <td>121</td>\n",
       "      <td>200 word wikipedia style introduction on 'Step...</td>\n",
       "      <td>and illustrated by Maki Fujii. The series fol...</td>\n",
       "      <td>26</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38894426</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Rob%20Bradley</td>\n",
       "      <td>Rob Bradley</td>\n",
       "      <td>Robert Milner \"Rob\" Bradley, Jr. (born August ...</td>\n",
       "      <td>Robert Milner \"Rob\" Bradley, Jr. (born August ...</td>\n",
       "      <td>2</td>\n",
       "      <td>170</td>\n",
       "      <td>136</td>\n",
       "      <td>200 word wikipedia style introduction on 'Rob ...</td>\n",
       "      <td>29, 1973) is an American former professional ...</td>\n",
       "      <td>28</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>44173767</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Randy%20Borum</td>\n",
       "      <td>Randy Borum</td>\n",
       "      <td>Randy Borum is a Professor and Coordinator of ...</td>\n",
       "      <td>Randy Borum is a Professor and Coordinator of ...</td>\n",
       "      <td>2</td>\n",
       "      <td>185</td>\n",
       "      <td>71</td>\n",
       "      <td>200 word wikipedia style introduction on 'Rand...</td>\n",
       "      <td>of the Master of Fine Arts Program in Creativ...</td>\n",
       "      <td>25</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>33564134</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Sa%27och%20language</td>\n",
       "      <td>Sa'och language</td>\n",
       "      <td>Sa'och (, also, \"Sauch\") is an endangered, nea...</td>\n",
       "      <td>Sa'och (, also, \"Sauch\") is an endangered, nuc...</td>\n",
       "      <td>2</td>\n",
       "      <td>175</td>\n",
       "      <td>134</td>\n",
       "      <td>200 word wikipedia style introduction on 'Sa'o...</td>\n",
       "      <td>nuclear-speaking, isolate language of the Ath...</td>\n",
       "      <td>33</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>4219548</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Philip%20Hanawalt</td>\n",
       "      <td>Philip Hanawalt</td>\n",
       "      <td>Philip C. Hanawalt (born 1931) is an American ...</td>\n",
       "      <td>Philip C. Hanawalt (born 1931) is an American ...</td>\n",
       "      <td>2</td>\n",
       "      <td>166</td>\n",
       "      <td>191</td>\n",
       "      <td>200 word wikipedia style introduction on 'Phil...</td>\n",
       "      <td>American graphic artist and illustrator. He i...</td>\n",
       "      <td>30</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>2625970</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Vossius%20Gymnasium</td>\n",
       "      <td>Vossius Gymnasium</td>\n",
       "      <td>Vossius Gymnasium is a public gymnasium in Ams...</td>\n",
       "      <td>Vossius Gymnasium is a public gymnasium in the...</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>108</td>\n",
       "      <td>200 word wikipedia style introduction on 'Voss...</td>\n",
       "      <td>the town of Vossius, Netherlands. It is named...</td>\n",
       "      <td>32</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>32345102</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Simone%20Stratigo</td>\n",
       "      <td>Simone Stratigo</td>\n",
       "      <td>Simone Stratigo (, Symeon Filippos Stratigos; ...</td>\n",
       "      <td>Simone Stratigo (, Symeon Filippos Stratigos; ...</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>132</td>\n",
       "      <td>200 word wikipedia style introduction on 'Simo...</td>\n",
       "      <td>born 1 July 1979) is a Greek professional foo...</td>\n",
       "      <td>33</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                                url  \\\n",
       "0       63064638  https://en.wikipedia.org/wiki/Sexhow%20railway...   \n",
       "1         279621        https://en.wikipedia.org/wiki/Eti%C3%A4inen   \n",
       "2         287229  https://en.wikipedia.org/wiki/Inverse%20functi...   \n",
       "3       26712375  https://en.wikipedia.org/wiki/Stepping%20on%20...   \n",
       "4       38894426        https://en.wikipedia.org/wiki/Rob%20Bradley   \n",
       "...          ...                                                ...   \n",
       "149995  44173767        https://en.wikipedia.org/wiki/Randy%20Borum   \n",
       "149996  33564134  https://en.wikipedia.org/wiki/Sa%27och%20language   \n",
       "149997   4219548    https://en.wikipedia.org/wiki/Philip%20Hanawalt   \n",
       "149998   2625970  https://en.wikipedia.org/wiki/Vossius%20Gymnasium   \n",
       "149999  32345102    https://en.wikipedia.org/wiki/Simone%20Stratigo   \n",
       "\n",
       "                           title  \\\n",
       "0         Sexhow railway station   \n",
       "1                       Etiäinen   \n",
       "2       Inverse function theorem   \n",
       "3              Stepping on Roses   \n",
       "4                    Rob Bradley   \n",
       "...                          ...   \n",
       "149995               Randy Borum   \n",
       "149996           Sa'och language   \n",
       "149997           Philip Hanawalt   \n",
       "149998         Vossius Gymnasium   \n",
       "149999           Simone Stratigo   \n",
       "\n",
       "                                               wiki_intro  \\\n",
       "0       Sexhow railway station was a railway station b...   \n",
       "1       In Finnish folklore, all places and things, an...   \n",
       "2       In mathematics, specifically differential calc...   \n",
       "3       is a Japanese shōjo manga series written and i...   \n",
       "4       Robert Milner \"Rob\" Bradley, Jr. (born August ...   \n",
       "...                                                   ...   \n",
       "149995  Randy Borum is a Professor and Coordinator of ...   \n",
       "149996  Sa'och (, also, \"Sauch\") is an endangered, nea...   \n",
       "149997  Philip C. Hanawalt (born 1931) is an American ...   \n",
       "149998  Vossius Gymnasium is a public gymnasium in Ams...   \n",
       "149999  Simone Stratigo (, Symeon Filippos Stratigos; ...   \n",
       "\n",
       "                                          generated_intro  title_len  \\\n",
       "0       Sexhow railway station was a railway station l...          3   \n",
       "1       In Finnish folklore, all places and things, an...          1   \n",
       "2       In mathematics, specifically differential calc...          3   \n",
       "3       is a Japanese shōjo manga series written and i...          3   \n",
       "4       Robert Milner \"Rob\" Bradley, Jr. (born August ...          2   \n",
       "...                                                   ...        ...   \n",
       "149995  Randy Borum is a Professor and Coordinator of ...          2   \n",
       "149996  Sa'och (, also, \"Sauch\") is an endangered, nuc...          2   \n",
       "149997  Philip C. Hanawalt (born 1931) is an American ...          2   \n",
       "149998  Vossius Gymnasium is a public gymnasium in the...          2   \n",
       "149999  Simone Stratigo (, Symeon Filippos Stratigos; ...          2   \n",
       "\n",
       "        wiki_intro_len  generated_intro_len  \\\n",
       "0                  174                   78   \n",
       "1                  187                   80   \n",
       "2                  170                   59   \n",
       "3                  335                  121   \n",
       "4                  170                  136   \n",
       "...                ...                  ...   \n",
       "149995             185                   71   \n",
       "149996             175                  134   \n",
       "149997             166                  191   \n",
       "149998             168                  108   \n",
       "149999             153                  132   \n",
       "\n",
       "                                                   prompt  \\\n",
       "0       200 word wikipedia style introduction on 'Sexh...   \n",
       "1       200 word wikipedia style introduction on 'Etiä...   \n",
       "2       200 word wikipedia style introduction on 'Inve...   \n",
       "3       200 word wikipedia style introduction on 'Step...   \n",
       "4       200 word wikipedia style introduction on 'Rob ...   \n",
       "...                                                   ...   \n",
       "149995  200 word wikipedia style introduction on 'Rand...   \n",
       "149996  200 word wikipedia style introduction on 'Sa'o...   \n",
       "149997  200 word wikipedia style introduction on 'Phil...   \n",
       "149998  200 word wikipedia style introduction on 'Voss...   \n",
       "149999  200 word wikipedia style introduction on 'Simo...   \n",
       "\n",
       "                                           generated_text  prompt_tokens  \\\n",
       "0        located in the town of Sexhow, on the Cumbria...             25   \n",
       "1        animate or inanimate, have a spirit or \"etiäi...             26   \n",
       "2        function theorem states that for every real-v...             26   \n",
       "3        and illustrated by Maki Fujii. The series fol...             26   \n",
       "4        29, 1973) is an American former professional ...             28   \n",
       "...                                                   ...            ...   \n",
       "149995   of the Master of Fine Arts Program in Creativ...             25   \n",
       "149996   nuclear-speaking, isolate language of the Ath...             33   \n",
       "149997   American graphic artist and illustrator. He i...             30   \n",
       "149998   the town of Vossius, Netherlands. It is named...             32   \n",
       "149999   born 1 July 1979) is a Greek professional foo...             33   \n",
       "\n",
       "        generated_text_tokens  \n",
       "0                          88  \n",
       "1                         101  \n",
       "2                          65  \n",
       "3                         150  \n",
       "4                         162  \n",
       "...                       ...  \n",
       "149995                     92  \n",
       "149996                    184  \n",
       "149997                    272  \n",
       "149998                    147  \n",
       "149999                    173  \n",
       "\n",
       "[150000 rows x 12 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f21b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "import textstat\n",
    "from collections import Counter\n",
    "import language_tool_python\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle \n",
    "import spacy\n",
    "# Load the English NER model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bebee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"sample121.csv\")\n",
    "df2 = pd.read_csv(\"sample12.csv\" , encoding =\"ISO-8859-1\")\n",
    "concatenated_df = pd.concat([df1, df2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47c38bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                            0\n",
       "generated                       0\n",
       "word_count_regex                0\n",
       "punctuation_count               0\n",
       "verb_count                      0\n",
       "adj_count                       0\n",
       "noun_count                      0\n",
       " average_sentence               0\n",
       "count_ner_tokens                0\n",
       "count_grammatical_errors        0\n",
       "hapax_legomena_count            0\n",
       "flesch_reading_ease             0\n",
       "flesch_kincaid_grade            0\n",
       "gunning_fog                     0\n",
       "coleman_liau_index              0\n",
       "dale_chall_readability_score    0\n",
       "ari                             0\n",
       "linsear_write_formula           0\n",
       "spache_readability              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "074cd20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear', probability=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = concatenated_df.iloc[:,list(range(2, concatenated_df.shape[1]))]\n",
    "Y = concatenated_df.iloc[:,1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "svm = SVC(kernel='linear',probability = True) \n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9e9bc67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.949"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = svm.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc74882e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.9595278246205734\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score, confusion_matrix\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6043bc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9386341141537446\n"
     ]
    }
   ],
   "source": [
    "# Calculate Precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(f\"Precision: {precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6ada90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 Score\n",
    "f1 = f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbe9c44e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9489659773182122"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f4879b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2849,  186],\n",
       "       [ 120, 2845]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74d0e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt=pickle.load(open('model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bdb9912",
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = gt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1660a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.918"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,gg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b113995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2845,  190],\n",
       "       [ 302, 2663]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, gg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6abc670",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"model1.pkl\"\n",
    "pickle.dump(svm,open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b181bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d487cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16fc8ba280144819e85db74234df256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PC\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d23a11ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(text):\n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "    # Compute the loss\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens, labels=tokens)\n",
    "        loss = outputs.loss.item()\n",
    "\n",
    "    # Calculate perplexity\n",
    "    perplexity = torch.exp(torch.tensor(loss)).item()\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f3c73197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 58.391963958740234\n"
     ]
    }
   ],
   "source": [
    "text= \"\"\"\"\n",
    "Dear Senator, Many people might agree theat we shouldn't have an Electoral College at all, since most of thee time it doesn't seem to work at all since of instead just voting on thee person or candidate you would prefer, you instead vote for a slate of electors instead of thee preferred candidate. This is not thee case, however, and in fact, thee Electoral College does seem to function. It can be proven in two simple reasons on why thee Electoral College still works certainty of outcome and thee simple fact theat it's everyone's president.\n",
    "\n",
    "Foremost, thee certainty of outcome can easily prove why thee Electoral College still works. If we even do go to a system where we actually vote for popular vote, it would work as well as we do right now withe thee Electoral College, since theere will be more disputes on popular vote thean on thee Electoral College. In Richard A. poster's article on thee defense for thee Electoral College, he states in his first reason on why theeir would be much dispute over popular vote thean on thee Electoral College, and why it's less likely to happen on a Electoral College. He states \"\"The reason is theat thee winning candidate's share of thee Electoral College invariably exceeds his share of thee popular vote.\"\" It's true, after all theat thee Electoral College's votes exceed over thee popular votes. It could be also said theat even if thee government actually switches to popular vote, we know it would work since it does not exceed how on Electoral College votes go, which can be also seen as a simpler process of voting. On anotheer note, in some cases, theere can be ties between two candidates, as seen in 1992's Election between Nixon and Clinton on thee popular vote. To summarize, thee certainty of outcome is far greater in thee Electoral College ratheer thean thee popular vote due to sheer number of votes in thee Electoral College.\n",
    "hould be kept due to thee simple reasoning of certainty of outcome and thee voting on thee people's president. The reasoning of certainty of outcome can be applied here is due to thee simple fact theat thee votes for thee electoral college are far more greater thean thee votes for popular vote, same goes to thee voting on thee people's president, since it focuses on thee entire nation ratheer theat just one region of thee country. Senator, we should be able to keep our electoral college, as it proves effective during thee recent elections, and can certainly prove itself useful and reliable during thee next elections to come.\n",
    "\"\"\"\n",
    "# Calculate perplexity\n",
    "perplexity = calculate_perplexity(text)\n",
    "print(f'Perplexity: {perplexity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3c761a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, BertForMaskedLM, BertTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def calculate_perplexities(text, gpt2_model_name='gpt2', bert_model_name='bert-base-uncased',max_length=512):\n",
    "    # Load GPT-2 model and tokenizer\n",
    "    gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_model_name)\n",
    "    gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model_name)\n",
    "    gpt2_model.eval()\n",
    "\n",
    "    # Load BERT model and tokenizer\n",
    "    bert_model = BertForMaskedLM.from_pretrained(bert_model_name)\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "    bert_model.eval()\n",
    "\n",
    "    # Tokenize the input text for GPT-2\n",
    "    gpt2_tokens = gpt2_tokenizer.encode(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        gpt2_outputs = gpt2_model(gpt2_tokens, labels=gpt2_tokens)\n",
    "        gpt2_loss = gpt2_outputs.loss.item()\n",
    "        gpt2_perplexity = np.exp(gpt2_loss)\n",
    "\n",
    "    # Tokenize the input text for BERT\n",
    "    bert_tokens = bert_tokenizer.encode(text, return_tensors='pt')\n",
    "    mask_token = bert_tokenizer.mask_token_id\n",
    "    masked_input = bert_tokens.clone()\n",
    "    mask_position = np.random.randint(1, masked_input.size(1) - 1)\n",
    "    masked_input[0, mask_position] = mask_token\n",
    "    with torch.no_grad():\n",
    "        bert_outputs = bert_model(masked_input, labels=bert_tokens)\n",
    "        bert_loss = bert_outputs.loss.item()\n",
    "        bert_perplexity = np.exp(bert_loss)\n",
    "\n",
    "    return gpt2_perplexity, bert_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5c2222",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_perplexities(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23cee82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6bf6f1e22724ff6a5d644e0f1d43a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PC\\.cache\\huggingface\\hub\\models--allenai--longformer-base-4096. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3df17e86834c12bbaed7d1db10dd08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/597M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57564f8c128f49abb9a81d07fdc7f52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eded358f1b8b4a899ceab2ab00f8de16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb719e576fb0484286435a70f0628877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 1210 to 1536 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1926239504948764"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LongformerForMaskedLM, LongformerTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def calculate_longformer_perplexity(text, model_name='allenai/longformer-base-4096', max_length=4096):\n",
    "    # Load Longformer model and tokenizer\n",
    "    model = LongformerForMaskedLM.from_pretrained(model_name)\n",
    "    tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer.encode(text, return_tensors='pt', max_length=max_length, truncation=True)\n",
    "    mask_token = tokenizer.mask_token_id\n",
    "    masked_input = tokens.clone()\n",
    "    mask_position = np.random.randint(1, masked_input.size(1) - 1)\n",
    "    masked_input[0, mask_position] = mask_token\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(masked_input, labels=tokens)\n",
    "        loss = outputs.loss.item()\n",
    "        perplexity = np.exp(loss)\n",
    "\n",
    "    return perplexity\n",
    "\n",
    "longformer_perplexity = calculate_longformer_perplexity(text)\n",
    "longformer_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa67cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
