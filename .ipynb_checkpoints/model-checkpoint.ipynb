{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05c7b33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "import textstat\n",
    "from collections import Counter\n",
    "import language_tool_python\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle \n",
    "import spacy\n",
    "# Load the English NER model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, BertForMaskedLM, BertTokenizer\n",
    "import torch\n",
    "import numpy as np \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88da60a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data22 = pd.read_csv(\"sample23.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71fe6b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Punctuation count\n",
    "def punctuation_count(text):\n",
    "    punctuation = re.findall(r'[^\\w\\s]',text)\n",
    "    return len(punctuation)\n",
    "\n",
    "#word count\n",
    "def word_count_regex(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    return len(words)\n",
    "\n",
    "#noun count\n",
    "def noun_count(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    noun_count = sum(1 for _, tag in tagged_tokens if tag.startswith('NN'))\n",
    "    return noun_count\n",
    "\n",
    "#verb count\n",
    "def verb_count(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    verb_count = sum(1 for _, tag in tagged_tokens if tag.startswith('VB'))\n",
    "    return verb_count\n",
    "\n",
    "#adjective count\n",
    "def adj_count(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    adj_count = sum(1 for _, tag in tagged_tokens if tag.startswith('JJ'))\n",
    "    return adj_count\n",
    "\n",
    "#Average sentence length\n",
    "def  average_sentence(text):\n",
    "    # Split the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    # Split each sentence into words and calculate the length of each sentence\n",
    "    sentence_lengths = [len(nltk.word_tokenize(sentence)) for sentence in sentences]\n",
    "    # Calculate the average sentence length\n",
    "    if sentence_lengths:\n",
    "        avg_length = sum(sentence_lengths) / len(sentence_lengths)\n",
    "    else:\n",
    "        avg_length = 0\n",
    "    return avg_length\n",
    "\n",
    "#number of named entity\n",
    "def count_ner_tokens(text):\n",
    "    doc = nlp(text)\n",
    "    ner_count = len(doc.ents)\n",
    "    return ner_count\n",
    "\n",
    "\n",
    "# function to calculate readability scores\n",
    "def calculate_readability_scores(text):\n",
    "    return pd.Series({\n",
    "        'flesch_reading_ease': textstat.flesch_reading_ease(text),\n",
    "        'flesch_kincaid_grade': textstat.flesch_kincaid_grade(text),\n",
    "        'gunning_fog': textstat.gunning_fog(text),\n",
    "        'coleman_liau_index': textstat.coleman_liau_index(text),\n",
    "        'dale_chall_readability_score': textstat.dale_chall_readability_score(text),\n",
    "        'ari': textstat.automated_readability_index(text),\n",
    "        'linsear_write_formula': textstat.linsear_write_formula(text),\n",
    "        'spache_readability': textstat.spache_readability(text)\n",
    "    })\n",
    "\n",
    "\n",
    "#number of grammatical errors \n",
    "def count_grammatical_errors(text):\n",
    "    matches = tool.check(text)\n",
    "    return len(matches)\n",
    "\n",
    "\n",
    "#number of unique words in a text \n",
    "def hapax_legomena_count(text):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "        # Return 0 if there are no words to avoid division by zero\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    # Count the frequency of each word\n",
    "    word_counts = Counter(words)\n",
    "    # Hapax legomena are words that appear exactly once\n",
    "    hapax_legomena = [word for word, count in word_counts.items() if count == 1]\n",
    "    hapax_ratio = len(hapax_legomena)/len(words)\n",
    "    return hapax_ratio\n",
    "\n",
    "\n",
    "def split_text(text, tokenizer, max_length):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = [tokens[i:i+max_length] for i in range(0, len(tokens), max_length)]\n",
    "    return chunks\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, tokens):\n",
    "    tokens_tensor = torch.tensor(tokens).unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, labels=tokens_tensor)\n",
    "        loss = outputs.loss.item()\n",
    "    return np.exp(loss)\n",
    "\n",
    "def calculate_perplexities(text, gpt2_model_name='gpt2', bert_model_name='bert-base-uncased', max_length=512, random_seed=42):\n",
    "    # Set random seed for consistency\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "\n",
    "    # Load GPT-2 model and tokenizer\n",
    "    gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_model_name)\n",
    "    gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model_name)\n",
    "    gpt2_model.eval()\n",
    "\n",
    "    # Load BERT model and tokenizer\n",
    "    bert_model = BertForMaskedLM.from_pretrained(bert_model_name)\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "    bert_model.eval()\n",
    "\n",
    "    # Split the input text into chunks for GPT-2\n",
    "    gpt2_chunks = split_text(text, gpt2_tokenizer, max_length=1024)\n",
    "    \n",
    "    # Calculate perplexity for each chunk and average them for GPT-2\n",
    "    gpt2_perplexities = [calculate_perplexity(gpt2_model, gpt2_tokenizer, chunk) for chunk in gpt2_chunks]\n",
    "    avg_gpt2_perplexity = np.mean(gpt2_perplexities)\n",
    "\n",
    "    # Split the input text into chunks for BERT\n",
    "    bert_chunks = split_text(text, bert_tokenizer, max_length=512)\n",
    "    \n",
    "    # Calculate perplexity for each chunk and average them for BERT\n",
    "    bert_perplexities = []\n",
    "    mask_token = bert_tokenizer.mask_token_id\n",
    "\n",
    "    for chunk in bert_chunks:\n",
    "        if len(chunk) > 2:  # Ensure there are enough tokens to mask\n",
    "            masked_chunk = chunk.copy()\n",
    "            mask_position = np.random.randint(1, len(masked_chunk) - 1)\n",
    "            masked_chunk[mask_position] = mask_token\n",
    "            bert_perplexity = calculate_perplexity(bert_model, bert_tokenizer, masked_chunk)\n",
    "            bert_perplexities.append(bert_perplexity)\n",
    "    \n",
    "    avg_bert_perplexity = np.mean(bert_perplexities) if bert_perplexities else float('inf')\n",
    "\n",
    "    return avg_gpt2_perplexity, avg_bert_perplexity\n",
    "\n",
    "\n",
    "def extract_features(text):\n",
    "    \n",
    "    gpt2_perplexity, bert_perplexity = calculate_perplexities(text)\n",
    "    # Call each feature extraction function and collect the results\n",
    "    features = {\n",
    "        \"word_count_regex\": word_count_regex(text),\n",
    "        \"punctuation_count\": punctuation_count(text),\n",
    "        \"verb_count\": verb_count(text),\n",
    "        \"adj_count\": adj_count(text),\n",
    "        \"noun_count\": noun_count(text),\n",
    "        \" average_sentence\": average_sentence(text),\n",
    "        \"count_ner_tokens\": count_ner_tokens(text),\n",
    "        \"count_grammatical_errors\": count_grammatical_errors(text),\n",
    "        \"hapax_legomena_count\": hapax_legomena_count(text),\n",
    "        **calculate_readability_scores(text), # Merge readability scores\n",
    "        \"gpt2_perplexity\": gpt2_perplexity,\n",
    "        \"bert_perplexity\": bert_perplexity\n",
    "        }\n",
    "    return features\n",
    "#features_df = pd.DataFrame(features, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1dc498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000  \n",
    "\n",
    "# Initialize an empty list to hold the processed chunks\n",
    "processed_chunks = []\n",
    "for chunk in pd.read_csv(\"sample23.csv\", chunksize=chunk_size):\n",
    "  # Apply the functions to the 'text' column\n",
    "   chunk['word_count_regex'] = chunk['text'].apply(word_count_regex)\n",
    "   chunk['punctuation_count'] = chunk['text'].apply(punctuation_count)\n",
    "   chunk['verb_count'] = chunk['text'].apply(verb_count)\n",
    "   chunk['adj_count'] = chunk['text'].apply(adj_count)\n",
    "   chunk['noun_count'] = chunk['text'].apply(noun_count)\n",
    "   chunk[' average_sentence'] = chunk['text'].apply(average_sentence)\n",
    "   chunk['count_ner_tokens'] = chunk['text'].apply(count_ner_tokens)\n",
    "   chunk['count_grammatical_errors'] = chunk['text'].apply(count_grammatical_errors)\n",
    "   chunk['hapax_legomena_count'] = chunk['text'].apply(hapax_legomena_count)\n",
    "   #append the processed chunk to the list\n",
    "   processed_chunks.append(chunk)\n",
    "\n",
    "# Concatenate all processed chunks into a single DataFrame\n",
    "processed_data = pd.concat(processed_chunks, ignore_index=True)\n",
    "# Combine the readability scores with the original data\n",
    "readability_scores_df = processed_data['text'].apply(calculate_readability_scores)\n",
    "readability_scores_df[['gpt2_perplexity', 'bert_perplexity']] = readability_scores_df['text'].apply(\n",
    "    lambda x: pd.Series(calculate_perplexities(x))\n",
    ")\n",
    "combined_df = pd.concat([processed_data, readability_scores_df], axis=1)\n",
    "combined_df.to_csv('final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19801abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset into X and Y\n",
    "X = combined_df.iloc[:,list(range(2, combined_df.shape[1]))]\n",
    "Y = combined_df.iloc[:,1]\n",
    "\n",
    "#splitting dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "#loading scaler \n",
    "scaler = StandardScaler()\n",
    "\n",
    "#scaling train dataset\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "#fitting the model \n",
    "svm = SVC(kernel='rbf',probability = True) \n",
    "svm.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72bfb2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model performance measure \n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_pred = svm.predict(X_test_scaled)\n",
    "\n",
    "#model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "#recall score\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# Precision score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30afd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = extract_features(x)\n",
    "features_df = pd.DataFrame(features, index=[0])\n",
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d94db2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
